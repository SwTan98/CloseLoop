{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive EN (5000, 7)\n",
      "Negative EN (5000, 6)\n",
      "Positive MY (4972, 1)\n",
      "Negative MY (4990, 3)\n"
     ]
    }
   ],
   "source": [
    "positive_en_dat = pd.read_fwf('bilanguage-subjectivity/positive-english', header=None)\n",
    "negative_en_dat = pd.read_fwf('bilanguage-subjectivity/negative-english', header=None)\n",
    "positive_my_dat = pd.read_fwf('bilanguage-subjectivity/subjectivity-positivity-bm.txt', header=None)\n",
    "negative_my_dat = pd.read_fwf('bilanguage-subjectivity/subjectivity-negative-bm.txt', header=None)\n",
    "print(\"Positive EN\", positive_en_dat.shape)\n",
    "print(\"Negative EN\", negative_en_dat.shape)\n",
    "print(\"Positive MY\", positive_my_dat.shape)\n",
    "print(\"Negative MY\", negative_my_dat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove NAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_en, negative_en, positive_my, negative_my = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "positive_en['sentence'] = positive_en_dat[0]\n",
    "negative_en['sentence'] = negative_en_dat[0]\n",
    "positive_my['sentence'] = positive_my_dat[0]\n",
    "negative_my['sentence'] = negative_my_dat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data(Subjective=1,Objective=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_en['subjective'] = [1 for i in range(len(positive_en))]\n",
    "negative_en['subjective'] = [0 for i in range(len(negative_en))]\n",
    "positive_my['subjective'] = [1 for i in range(len(positive_my))]\n",
    "negative_my['subjective'] = [0 for i in range(len(negative_my))]\n",
    "\n",
    "positive_en['lang'] = [\"en\" for i in range(len(positive_en))]\n",
    "negative_en['lang'] = [\"en\" for i in range(len(negative_en))]\n",
    "positive_my['lang'] = [\"my\" for i in range(len(positive_my))]\n",
    "negative_my['lang'] = [\"my\" for i in range(len(negative_my))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = positive_en.append(negative_en, ignore_index=True).append(positive_my, ignore_index=True).append(negative_my, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemmer code block copied from https://github.com/huseinzol05/Malaya/blob/master/session/emotion/multinomial.ipynb\n",
    "permulaan = [\n",
    "    'bel',\n",
    "    'se',\n",
    "    'ter',\n",
    "    'men',\n",
    "    'meng',\n",
    "    'mem',\n",
    "    'memper',\n",
    "    'di',\n",
    "    'pe',\n",
    "    'me',\n",
    "    'ke',\n",
    "    'ber',\n",
    "    'pen',\n",
    "    'per',\n",
    "]\n",
    "\n",
    "hujung = ['kan', 'kah', 'lah', 'tah', 'nya', 'an', 'wan', 'wati', 'ita']\n",
    "\n",
    "def naive_stemmer(word):\n",
    "    assert isinstance(word, str), 'input must be a string'\n",
    "    hujung_result = re.findall(r'^(.*?)(%s)$' % ('|'.join(hujung)), word)\n",
    "    word = hujung_result[0][0] if len(hujung_result) else word\n",
    "    permulaan_result = re.findall(r'^(.*?)(%s)' % ('|'.join(permulaan[::-1])), word)\n",
    "    permulaan_result.extend(re.findall(r'^(.*?)(%s)' % ('|'.join(permulaan)), word))\n",
    "    mula = permulaan_result if len(permulaan_result) else ''\n",
    "    if len(mula):\n",
    "        mula = mula[1][1] if len(mula[1][1]) > len(mula[0][1]) else mula[0][1]\n",
    "    return word.replace(mula, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "stop_en = set(stopwords.words('english'))\n",
    "stop_my = set(stopwords.words('indonesian'))\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "def clean(sentence):\n",
    "    cleaned = \"\"\n",
    "    # lower, remove punctuation, stop words, lemmatize\n",
    "    if detect(sentence)==\"en\":\n",
    "        tokenized = [word for word in word_tokenize(sentence.lower().translate(str.maketrans('','',string.punctuation))) if word not in stop_en]\n",
    "        for word in tokenized:\n",
    "            cleaned += porter_stemmer.stem(word) + \" \"\n",
    "        return cleaned\n",
    "    else:\n",
    "        tokenized = [word for word in word_tokenize(sentence.lower().translate(str.maketrans('','',string.punctuation))) if word not in stop_my]\n",
    "        for word in tokenized:\n",
    "            cleaned += naive_stemmer(word) + \" \"\n",
    "        return cleaned\n",
    "\n",
    "%time data['sentence'] = data['sentence'].apply(clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19962, 31016)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(ngram_range=(1,3),min_df=2).fit(data['sentence'])\n",
    "vectors = tfidf.transform(data['sentence'])\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_Y, test_Y = train_test_split(vectors, data['subjective'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   objective       0.97      0.95      0.96      8012\n",
      "  subjective       0.95      0.97      0.96      7957\n",
      "\n",
      "   micro avg       0.96      0.96      0.96     15969\n",
      "   macro avg       0.96      0.96      0.96     15969\n",
      "weighted avg       0.96      0.96      0.96     15969\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multinomial  = MultinomialNB().fit(train_X, train_Y)\n",
    "print(\n",
    "    metrics.classification_report(\n",
    "        train_Y,\n",
    "        multinomial.predict(train_X),\n",
    "        target_names = ['objective','subjective'],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    \"\"\"\n",
    "    Will predict subjectivity after cleaning the text\n",
    "    Output:\n",
    "    Text\n",
    "    Subjective: True/False  Confidence: int\n",
    "    \"\"\"\n",
    "    if text.strip(): # whitespace/no text\n",
    "        clean_text = tfidf.transform([clean(text)])\n",
    "        print(text)\n",
    "        print(\"Subjective:\", True if multinomial.predict(clean_text)[0] else False, \"\\tConfidence :\", multinomial.predict_proba(clean_text)[0].max())\n",
    "        print()\n",
    "    else:\n",
    "        print(\"Please enter text to predict subjectivity.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essenti film weak detail strong person \n",
      "Subjective: True \tConfidence : 0.9031010185616164\n",
      "\n",
      "endear hear madam refer husband jacki make excel compani least selfconsci perform \n",
      "Subjective: True \tConfidence : 0.7632514331607971\n",
      "\n",
      "marri that suppos \n",
      "Subjective: False \tConfidence : 0.9123177462247096\n",
      "\n",
      "mafiosi amino gino ttimo lindung kafe glasgow miliki pupu scottishitali \n",
      "Subjective: False \tConfidence : 0.8723574576331053\n",
      "\n",
      "graciou eloqu film end offer ray hope refuge abl look ahead resist live past forev lost \n",
      "Subjective: False \tConfidence : 0.5186475692854761\n",
      "\n",
      "this is a movie about a man and his drive to succeed\n",
      "Subjective: False \tConfidence : 0.5172754947302303\n",
      "\n",
      "filem itu menerangkan seorang penyanyi yang mengalami kehidupan yang susah\n",
      "Subjective: False \tConfidence : 0.5052014884137602\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test prediction on sample rows\n",
    "import random\n",
    "for i in range(5):\n",
    "    text=data['sentence'][random.randint(0,len(data))] # get sentence from random row\n",
    "    predict(text)\n",
    "predict(\"this is a movie about a man and his drive to succeed\")\n",
    "predict(\"filem itu menerangkan seorang penyanyi yang mengalami kehidupan yang susah\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('multinomial-subjectivity.pkl','wb') as fopen:\n",
    "    pickle.dump(multinomial,fopen)\n",
    "with open('tfidf-multinomial-subjectivity.pkl','wb') as fopen:\n",
    "    pickle.dump(tfidf,fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
